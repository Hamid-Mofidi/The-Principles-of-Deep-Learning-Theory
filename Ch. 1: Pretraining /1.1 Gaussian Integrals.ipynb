{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZ9MePxGOCTgGo8xEV0y0V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamid-Mofidi/The-Principles-of-Deep-Learning-Theory/blob/main/Ch.%201%3A%20Pretraining%20/1.1%20Gaussian%20Integrals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this book is to develop principles that enable a theoretical understanding of deep learning. **Perhaps the most important principle is that wide and deep neural networks are governed by nearly-Gaussian distributions.** Thus, to make it through the book, you will need to achieve mastery of **Gaussian integration** and **perturbation theory**."
      ],
      "metadata": {
        "id": "WmFw9qmuu9pE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We begin in $§1.1$ with an extended discussion of Gaussian integrals.\n",
        "Our emphasis will be on calculational tools for computing averages of monomials against Gaussian distributions, culminating in a derivation of Wick’s theorem.\n",
        "\n",
        "Next, in $§1.2$, we begin by giving a general discussion of expectation values and observables. \n",
        "\n",
        "In $§1.3$, we introduce the negative log probability or action representation of a probability distribution and explain how the action lets us systematically deform Gaussian distributions in order to give a compact representation of non-Gaussian distributions.\n",
        "\n",
        "In particular, we specialize to **nearly-Gaussian** distributions, for which deviations from Gaussianity are implemented by small couplings in the action, and show how perturbation theory can be used to connect the non-Gaussian couplings to observables such as the connected correlators. **By treating such couplings perturbatively, we can transform any correlator of a nearly-Gaussian distribution into a sum of Gaussian integrals**; each integral can then be evaluated by the tools **we developed in §1.1**. This will be one of our most important tricks, as the neural networks we’ll study are all governed by nearly-\n",
        "Gaussian distributions, with non-Gaussian couplings that become perturbatively small as the networks become wide."
      ],
      "metadata": {
        "id": "__L-ggx-wPzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Gaussian Integrals\n",
        "\n",
        "# Single-variable Gaussian integrals\n",
        "\n",
        "The simplest single-variable Gaussian function is,\n",
        "$$\n",
        "e^{-\\frac{z^2}{2}}  \\hspace{2in} (1.1)\n",
        "$$\n",
        "\n",
        "The Gaussian integral is, \n",
        "$$\n",
        "I_1 = \\int_{-∞}^{∞} dz ~~e^{-\\frac{z^2}{2}}  = \\sqrt {2 π} \\hspace{2in} (1.5)\n",
        "$$\n",
        "\n",
        "Dividing the Gaussian function (1.1) with this normalization factor, we define the **Gaussian probability distribution** with **unit variance** as\n",
        "$$\n",
        "p(z) ≡ \\dfrac{1}{\\sqrt {2 π} } e^{-\\frac{z^2}{2}}  \\hspace{2in} (1.6)\n",
        "$$\n",
        "which is now properly normalized, i.e., $\\int_{-∞}^{∞} dz ~p(z)= 1$. Such a distribution with **zero mean** and **unit variance** is sometimes called the **standard normal distribution**."
      ],
      "metadata": {
        "id": "r9CZKSiHyLLw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extending this result to a Gaussian distribution with variance $K > 0$ is super-easy. The corresponding normalization factor is given by\n",
        "$$\n",
        "I_K = \\int_{-∞}^{∞} dz ~~e^{-\\frac{z^2}{2K}}  = \\sqrt {2 πK}. \\hspace{2in} (1.7)\n",
        "$$\n",
        "\n",
        "We can then define the Gaussian distribution with variance $K$ as\n",
        "$$\n",
        "p(z) ≡ \\dfrac{1}{\\sqrt {2 πK} } e^{-\\frac{z^2}{2K}}.  \\hspace{2in} (1.8)\n",
        "$$\n",
        "\n",
        "More generally, we can shift the center of the bell curve as\n",
        "$$\n",
        "p(z) ≡ \\dfrac{1}{\\sqrt {2 πK} } e^{-\\frac{(z-s)^2}{2K}},  \\hspace{2in} (1.9)\n",
        "$$\n",
        "so that it is now symmetric around $z = s$. This center value $s$ is called the **mean of the distribution**, because the expected value of $z$ is equal to $s$, i.e., $E(z) = s$."
      ],
      "metadata": {
        "id": "AqkksZj70dKb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Focusing on Gaussian distributions (1.7) with zero mean, let’s consider other expectation values for general functions $\\mathcal{O}(z)$, i.e.,\n",
        "$$\n",
        "E[O(z)] = \\dfrac{1}{\\sqrt {2 πK} } \\int_{-∞}^{∞} dz ~~e^{-\\frac{z^2}{2K}} \\mathcal{O}(z)   \\hspace{1in} (1.11)\n",
        "$$\n",
        "We’ll often refer to such functions $\\mathcal{O}(z)$ as **observables**, since they can correspond to measurement outcomes of experiments. A special class of expectation values are called **moments** and correspond to the insertion of $z^M$ into the integrand for any integer $M$. The moment integral vanishes for any odd exponent $M$, because then the integrand is **odd** with respect to the sign flip $z \\leftrightarrow -z$. As for the even number $M = 2m$ of $z$\n",
        "insertions, we have,\n",
        "$$\n",
        "\\begin{aligned}\n",
        "I_{K,m} &≡ \\int_{-∞}^{∞} dz ~~e^{-\\frac{z^2}{2K}} ~z^{2m} \\hspace{2.45in} (1.13) \\\\\n",
        "&= \\sqrt{2\\pi} K^{\\frac{2m+1}{2}}(2m-1)(2m-3)\\cdots 1. \\hspace{1in} (1.14)\n",
        "\\end{aligned}\n",
        "$$\n",
        "Therefore, we see that the even moments are given by the simple formula\n",
        "\n",
        "$$\n",
        "E[z^{2m}] = \\dfrac{I_{K,m}}{\\sqrt{2\\pi K}} = K^m (2m-1)!! \\hspace{1in} (1.15)\n",
        "$$\n",
        "The result (1.15) is **Wick’s theorem for single-variable Gaussian distributions**.\n"
      ],
      "metadata": {
        "id": "8MpCAjAZ5FuZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There’s actually another nice way to derive (1.15), which can much more naturally be **extended to multivariable Gaussian distributions**. This derivation starts with the consideration of **a Gaussian integral with a source term J**, which we define as\n",
        "$$\n",
        "Z_{K,J} ≡ \\int_{-∞}^{+∞}dz~ e^{-\\frac{z^2}{2K}+Jz}. \\hspace{1in} (1.17)\n",
        "$$\n",
        "Note that when setting the source to zero we recover the normalization of the Gaussian integral, giving the relationship $Z_{K,J=0} = I_K$. In the physics literature $Z_{K,J}$ is sometimes called a **partition function with source** and, as we will soon see, this integral serves as a generating function for the moments. We can evaluate $Z_{K,J}$ by completing the square in the exponent, which lets us rewrite the integral (1.17) as\n",
        "$$\\begin{aligned}\n",
        "Z_{K,J} ≡& e^{\\frac{KJ^2}{2}}\\int_{-∞}^{+∞}dz~ e^{\\frac{(z-JK)^2}{2K}} \\\\\n",
        " = & e^{\\frac{KJ^2}{2}} I_K\\\\\n",
        "  = & e^{\\frac{KJ^2}{2}} \\sqrt{2\\pi K}. \\hspace{1in} (1.19)\n",
        "\\end{aligned}\n",
        "$$\n",
        "We can now relate the Gaussian integral with a source $Z_{K,J}$ to the Gaussian integral with insertions $I_{K,m}$. By differentiating $Z_{K,J}$ with respect to the source $J$ and then setting the source to zero, we observe that\n",
        "$$\n",
        "\\begin{aligned}\n",
        "I_{K,m} &≡ \\int_{-∞}^{∞} dz ~~e^{-\\frac{z^2}{2K}} ~z^{2m}  \\\\\n",
        "& = \\Big[\\big( \\dfrac{d}{dJ}\\big)^{2m} \\int_{-∞}^{∞} dz ~~e^{-\\frac{z^2}{2K}+Jz}    \\Big]{\\Big|}_{J=0} \\\\\n",
        "& = \\Big[\\big( \\dfrac{d}{dJ}\\big)^{2m} Z_{K,J}    \\Big]{\\Big|}_{J=0}  \\hspace{2.45in} (1.20)\n",
        "\\end{aligned}\n",
        "$$\n",
        "In other words, the integrals $I_{K,m}$ are simply related to the even Taylor coefficients of the partition function $Z_{K,J}$ around $J = 0$. For instance, for $2m = 2$ we have\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& E[z^2] = \\dfrac{I_{K,1}}{\\sqrt{2\\pi K}}= \\Big[\\big( \\dfrac{d}{dJ}\\big)^{2} e^{\\frac{KJ^2}{2}}    \\Big]{\\Big|}_{J=0} =K  \\hspace{1in} (1.21)\n",
        "\\end{aligned}\n",
        "$$\n",
        "Similarly $E[z^{2m}]$ can be obtained which completes our second derivation of Wick’s theorem (1.15) for the single-variable Gaussian distribution. This derivation was much longer than the first neat derivation, but can be very naturally **extended to the multivariable Gaussian distribution**, which\n",
        "we turn to next."
      ],
      "metadata": {
        "id": "WYIxdww0XUiX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multivariable Gaussian integrals\n",
        "\n",
        "The multivariable Gaussian function is\n",
        "defined as,\n",
        "$$\n",
        "f(z) = \\exp \\Big[-\\frac{1}{2} ∑_{\\mu , \\nu =1}^{N} z_{\\mu}(K^{-1})_{\\mu \\nu} z_\\nu  \\Big], \\hspace{1.5in} (1.24)\n",
        "$$\n",
        "where the variance or covariance matrix $K_{\\mu \\nu}$ is an $N\\times N$ **symmetric positive definite matrix**.  Now, to construct a probability distribution from the Gaussian function (1.24), we again need to evaluate the normalization factor\n",
        "$$\n",
        "\\begin{aligned}\n",
        "I_K &= \\int d^N z ~\\exp \\Big[-\\frac{1}{2} ∑_{\\mu , \\nu =1}^{N} z_{\\mu}(K^{-1})_{\\mu \\nu} z_\\nu  \\Big] \\\\\n",
        " & = \\sqrt{2\\pi |K|} = \\sqrt{2\\pi \\Pi_{\\mu =1}^N \\lambda_\\mu}, \\hspace{2.4in} (1.30)\n",
        "\\end{aligned}\n",
        "$$\n",
        "where $|K|$ denotes the determinant of a square matrix $K$, which is equal to  the product of the eigenvalues of matrix $K$.\n",
        "\n",
        "Having figured out the normalization factor, we can define the zero-mean multivariable Gaussian probability distribution with variance $K_{\\mu \\nu}$ as\n",
        "$$\n",
        "p(z) = \\dfrac{1}{\\sqrt{2\\pi |K|}} ~\\exp \\Big[-\\frac{1}{2} ∑_{\\mu , \\nu =1}^{N} z_{\\mu}K^{\\mu \\nu} z_\\nu  \\Big]  \\hspace{1in} (1.34)\n",
        "$$\n",
        "with the notation \n",
        "$$\n",
        "K^{\\mu \\nu} = (K^{-1})_{\\mu \\nu}. \\hspace{2in} (1.32)\n",
        "$$"
      ],
      "metadata": {
        "id": "_j9nGH3SBNi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let’s consider the moments of the mean-zero multivariable Gaussian distribution\n",
        "$$\n",
        "\\begin{aligned}\n",
        "E[z_{\\mu_1},\\cdots, z_{\\mu_M}] &≡ \\int d^Nz ~p(z)~ z_{\\mu_1} \\cdots  z_{\\mu_M}\\\\\n",
        "&= \\dfrac{I_{K,(\\mu_1,\\cdots,\\mu_M)}}{I_K}, \\hspace{1.1in} (1.36)\n",
        "\\end{aligned}\n",
        "$$\n",
        "where $I_K$ was defined in (1.30) and,\n",
        "$$\n",
        "I_{K,(\\mu_1,\\cdots,\\mu_M)} ≡ \\int d^Nz    ~\\exp \\Big[-\\frac{1}{2} ∑_{\\mu , \\nu =1}^{N} z_{\\mu}K^{\\mu \\nu} z_\\nu  \\Big]   ~ z_{\\mu_1} \\cdots  z_{\\mu_M} \\quad (1.37)\n",
        "$$"
      ],
      "metadata": {
        "id": "GQu3V9qJJF6q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following our approach in the single-variable case, let’s construct the generating function for the integrals $I_{K,(μ_1,...,μ_M)}$ by including a **source term** $J_μ$ as\n",
        "$$\\begin{aligned}\n",
        "Z_{K,J} &≡ \\int d^Nz    ~\\exp \\Big[-\\frac{1}{2} ∑_{\\mu , \\nu =1}^{N} z_{\\mu}K^{\\mu \\nu} z_\\nu  + ∑_{\\mu  =1}^{N} J^μ z_μ \\Big]  \\quad (1.38)\\\\\n",
        "&=\\sqrt{2\\pi |K|} \\exp  \\Big(\\frac{1}{2} ∑_{\\mu , \\nu =1}^{N} J^μ K_{\\mu\\nu}J^ν \\Big). \\hspace{1.5in} (1.41)\n",
        "\\end{aligned}\n",
        "$$\n",
        "With our closed-form expression (1.41) for the generating function ZK,J , we can compute the Gaussian integrals with insertions $I_{K,(μ_1,\\cdots,μ_M)}$ by differentiating it. For an even number $M = 2m$ of insertions, we find a really nice formula\n",
        "$$\n",
        "E[z_{\\mu_1},\\cdots, z_{\\mu_{2m} } ] = \\dfrac{1}{2^m m!}\\frac{d}{dJ^{\\mu_1}}  \\cdots \\frac{d}{dJ^{\\mu_{2m}}} \\Big( ∑_{\\mu , \\nu =1}^{N} J^\\mu K_{\\mu \\nu} J^\\nu\\Big). (1.42)\n",
        "$$\n",
        "For an odd number $M = 2m + 1$ of insertions, there is dangling source upon setting $J = 0$, and so those integrals vanish. You can also see this by looking at the integrand for any odd moment and noticing that it is odd with respect to the sign flip of the integration variables $z_μ ↔ −z_μ$.  \n"
      ],
      "metadata": {
        "id": "061W-I422n1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let’s take a few moments to evaluate a few moments using this formula. For\n",
        "2m = 2, we have\n",
        "$$\n",
        "E[z_{\\mu_1}z_{\\mu_2}] = \\dfrac{1}{2}\\frac{d}{dJ^{\\mu_1}}  \\frac{d}{dJ^{\\mu_{2}}} \\Big( ∑_{\\mu , \\nu =1}^{N} J^\\mu K_{\\mu \\nu} J^\\nu\\Big) = K_{\\mu_1\\mu_2}  \\hspace{.5in} (1.43)\n",
        "$$\n",
        "Here, there are $2! = 2$ ways to apply the product rule for derivatives and differentiate the two $J’s$, both of which evaluate to the same expression due to the symmetry of the covariance, $K_{μ_1μ_2} = K_{μ_2μ_1}$ . This expression (1.43) validates in the multivariable setting why we have been calling **$K_{μν}$ the covariance**, because we see explicitly that it is the covariance.\n",
        "\n",
        "Next, for $2m = 4$ we get a more complicated expression\n",
        "$$\n",
        "E[z_{\\mu_1}z_{\\mu_2}z_{\\mu_3}z_{\\mu_4}] = K_{\\mu_1\\mu_2}K_{\\mu_3\\mu_4} + K_{\\mu_1\\mu_3}K_{\\mu_2\\mu_4} +   K_{\\mu_1\\mu_4}K_{\\mu_2\\mu_3} \\hspace{.3in} (1.44)\n",
        "$$\n",
        "Here we note that there are now $4! = 24$ ways to differentiate the four $J’s$, though only three distinct ways to pair the four auxiliary indices $1, 2, 3, 4$ that sit under $μ$. This gives $24/3 = 8 = 2^2 2!$ equivalent terms for each of the three pairings, which cancels against the overall factor $1/(2^22!$).\n",
        "\n",
        "For general $2m$, there are $(2m)!$ ways to differentiate the sources, of which $2^mm!$ of those ways are equivalent. This gives $(2m)!/(2^mm!) = (2m − 1)!! $ distinct terms, corresponding to the $(2m − 1)!!$ distinct pairings of $2m$ auxiliary indices $1, \\cdots , 2m$ that sit under $μ$. The factor of $1/(2^mm!)$ in the denominator of (1.42) ensures that the coefficient of each of these terms is normalized to unity. Thus, most generally, we can\n",
        "express the moments of the multivariable Gaussian with the following formula\n",
        "$$\n",
        "E[z_{\\mu_1}\\cdots z_{\\mu_{2m}}] =∑_{\\text{all pairing}} K_{\\mu_{k_1}\\mu_{k_2}} \\cdots   K_{\\mu_{k_{2m-1}}\\mu_{k_{2m}}} \\hspace{.3in} (1.45)\n",
        "$$\n",
        "where, to reiterate, the sum is over all the possible distinct pairings of the $2m$ auxiliary indices under $μ$ such that the result has the $(2m − 1)!!$ terms that we described above. Each factor of the covariance $K_{μν}$ in a term in sum is **called a Wick contraction**, corresponding to a particular pairing of auxiliary indices. Each term then is composed of $m$ different Wick contractions, representing a distinct way of pairing up all the auxiliary\n",
        "indices.\n",
        "\n",
        "The formula (1.45) is **Wick’s theorem**. Put a box around it. Take a few moments\n",
        "for reflection."
      ],
      "metadata": {
        "id": "g31IcNTtCcnS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0WM4uTxtaQd"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ]
}